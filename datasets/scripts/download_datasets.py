#!/usr/bin/env python3
"""
REAP-OMNI å¤šæ¨¡æ€æ•°æ®é›†ä¸‹è½½ä¸è½¬æ¢è„šæœ¬

æœ¬è„šæœ¬å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
1. è‡ªåŠ¨ä¸‹è½½æ”¯æŒçš„æ•°æ®é›†
2. å°†æ‰€æœ‰æ•°æ®è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
3. ç”Ÿæˆ JSONL æ ¼å¼çš„æ ¡å‡†æ•°æ®

ç»Ÿä¸€è¾“å‡ºæ ¼å¼:
{
    "id": "dataset_name_00001",
    "text": "æºæ–‡æœ¬ï¼ˆå¦‚æœ‰ï¼‰",
    "audio": "path/to/audio.wav",  # å¦‚æœ‰
    "video": "path/to/video.mp4"   # å¦‚æœ‰
}

Usage:
    python download_datasets.py --dataset librispeech --output ./data --samples 100
    python download_datasets.py --dataset all --output ./data --samples 100
    python download_datasets.py --list  # åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†
"""

import os
import json
import argparse
import hashlib
import subprocess
from pathlib import Path
from typing import Optional, Dict, List, Any, Generator
from dataclasses import dataclass, field, asdict
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class UnifiedSample:
    """ç»Ÿä¸€æ•°æ®æ ¼å¼"""
    id: str
    text: Optional[str] = None
    audio: Optional[str] = None
    video: Optional[str] = None
    modality: str = "mixed"  # audio, video, mixed
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸ï¼Œæ’é™¤Noneå€¼"""
        result = {"id": self.id, "modality": self.modality}
        if self.text is not None:
            result["text"] = self.text
        if self.audio is not None:
            result["audio"] = self.audio
        if self.video is not None:
            result["video"] = self.video
        return result


class DatasetDownloader(ABC):
    """æ•°æ®é›†ä¸‹è½½å™¨åŸºç±»"""
    
    name: str = "base"
    modality: str = "mixed"  # audio, video, mixed
    description: str = ""
    url: str = ""
    
    def __init__(self, output_dir: Path, max_samples: int = 100):
        self.output_dir = output_dir
        self.max_samples = max_samples
        self.data_dir = output_dir / self.name
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
    @abstractmethod
    def download(self) -> bool:
        """ä¸‹è½½æ•°æ®é›†ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
        pass
    
    @abstractmethod
    def convert(self) -> Generator[UnifiedSample, None, None]:
        """è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼ï¼Œyield UnifiedSample"""
        pass
    
    def process(self) -> List[UnifiedSample]:
        """å®Œæ•´å¤„ç†æµç¨‹"""
        logger.info(f"Processing dataset: {self.name}")
        
        if not self.download():
            logger.error(f"Failed to download {self.name}")
            return []
        
        samples = []
        for i, sample in enumerate(self.convert()):
            if i >= self.max_samples:
                break
            samples.append(sample)
            
        logger.info(f"Processed {len(samples)} samples from {self.name}")
        return samples


# ============== Audio-only Datasets ==============

class LibriSpeechDownloader(DatasetDownloader):
    """LibriSpeech ASR æ•°æ®é›†"""
    
    name = "librispeech"
    modality = "audio"
    description = "è‹±è¯­æœ—è¯»è¯­éŸ³è¯†åˆ«æ•°æ®é›†ï¼Œæ¥è‡ªLibriVoxæœ‰å£°è¯»ç‰©"
    url = "https://huggingface.co/datasets/openslr/librispeech_asr"
    
    def download(self) -> bool:
        try:
            from datasets import load_dataset
            logger.info("Loading LibriSpeech from HuggingFace...")
            self.dataset = load_dataset(
                "openslr/librispeech_asr", 
                "clean",
                split="train.100",
                trust_remote_code=True
            )
            return True
        except Exception as e:
            logger.error(f"Error loading LibriSpeech: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        import soundfile as sf
        
        audio_dir = self.data_dir / "audio"
        audio_dir.mkdir(exist_ok=True)
        
        for idx, item in enumerate(self.dataset):
            if idx >= self.max_samples:
                break
                
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            audio_path = audio_dir / f"{self.name}_{idx:05d}.wav"
            audio_array = item["audio"]["array"]
            sample_rate = item["audio"]["sampling_rate"]
            sf.write(str(audio_path), audio_array, sample_rate)
            
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=item["text"],
                audio=str(audio_path),
                modality="audio"
            )


class CommonVoiceDownloader(DatasetDownloader):
    """Mozilla Common Voice æ•°æ®é›†"""
    
    name = "common_voice"
    modality = "audio"
    description = "å¤šè¯­è¨€ä¼—åŒ…è¯­éŸ³è¯†åˆ«æ•°æ®é›†"
    url = "https://huggingface.co/datasets/mozilla-foundation/common_voice_15_0"
    
    def download(self) -> bool:
        try:
            from datasets import load_dataset
            logger.info("Loading Common Voice from HuggingFace...")
            # åŠ è½½è‹±è¯­å­é›†
            self.dataset = load_dataset(
                "mozilla-foundation/common_voice_15_0",
                "en",
                split="train",
                trust_remote_code=True
            )
            return True
        except Exception as e:
            logger.error(f"Error loading Common Voice: {e}")
            logger.info("Common Voice requires login. Please run: huggingface-cli login")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        import soundfile as sf
        
        audio_dir = self.data_dir / "audio"
        audio_dir.mkdir(exist_ok=True)
        
        for idx, item in enumerate(self.dataset):
            if idx >= self.max_samples:
                break
            
            try:
                audio_path = audio_dir / f"{self.name}_{idx:05d}.wav"
                audio_array = item["audio"]["array"]
                sample_rate = item["audio"]["sampling_rate"]
                sf.write(str(audio_path), audio_array, sample_rate)
                
                yield UnifiedSample(
                    id=f"{self.name}_{idx:05d}",
                    text=item["sentence"],
                    audio=str(audio_path),
                    modality="audio"
                )
            except Exception as e:
                logger.warning(f"Error processing sample {idx}: {e}")
                continue


class GigaSpeechDownloader(DatasetDownloader):
    """GigaSpeech å¤§è§„æ¨¡ASRæ•°æ®é›†"""
    
    name = "gigaspeech"
    modality = "audio"
    description = "10000å°æ—¶å¤šé¢†åŸŸè‹±è¯­ASRæ•°æ®é›†"
    url = "https://huggingface.co/datasets/speechcolab/gigaspeech"
    
    def download(self) -> bool:
        try:
            from datasets import load_dataset
            logger.info("Loading GigaSpeech XS subset from HuggingFace...")
            self.dataset = load_dataset(
                "speechcolab/gigaspeech",
                "xs",  # ä½¿ç”¨æœ€å°å­é›†
                split="train",
                trust_remote_code=True
            )
            return True
        except Exception as e:
            logger.error(f"Error loading GigaSpeech: {e}")
            logger.info("GigaSpeech requires agreement. Visit the HuggingFace page to accept terms.")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        import soundfile as sf
        
        audio_dir = self.data_dir / "audio"
        audio_dir.mkdir(exist_ok=True)
        
        for idx, item in enumerate(self.dataset):
            if idx >= self.max_samples:
                break
            
            try:
                audio_path = audio_dir / f"{self.name}_{idx:05d}.wav"
                audio_array = item["audio"]["array"]
                sample_rate = item["audio"]["sampling_rate"]
                sf.write(str(audio_path), audio_array, sample_rate)
                
                yield UnifiedSample(
                    id=f"{self.name}_{idx:05d}",
                    text=item["text"],
                    audio=str(audio_path),
                    modality="audio"
                )
            except Exception as e:
                logger.warning(f"Error processing sample {idx}: {e}")
                continue


class WavCapsDownloader(DatasetDownloader):
    """WavCaps éŸ³é¢‘æè¿°æ•°æ®é›†"""
    
    name = "wavcaps"
    modality = "audio"
    description = "ChatGPTè¾…åŠ©çš„éŸ³é¢‘æè¿°æ•°æ®é›†"
    url = "https://huggingface.co/datasets/cvssp/WavCaps"
    
    def download(self) -> bool:
        try:
            from datasets import load_dataset
            logger.info("Loading WavCaps from HuggingFace...")
            self.dataset = load_dataset(
                "cvssp/WavCaps",
                split="train",
                trust_remote_code=True
            )
            return True
        except Exception as e:
            logger.error(f"Error loading WavCaps: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        import soundfile as sf
        
        audio_dir = self.data_dir / "audio"
        audio_dir.mkdir(exist_ok=True)
        
        for idx, item in enumerate(self.dataset):
            if idx >= self.max_samples:
                break
            
            try:
                audio_path = audio_dir / f"{self.name}_{idx:05d}.wav"
                if "audio" in item and item["audio"] is not None:
                    audio_array = item["audio"]["array"]
                    sample_rate = item["audio"]["sampling_rate"]
                    sf.write(str(audio_path), audio_array, sample_rate)
                    
                    yield UnifiedSample(
                        id=f"{self.name}_{idx:05d}",
                        text=item.get("caption", ""),
                        audio=str(audio_path),
                        modality="audio"
                    )
            except Exception as e:
                logger.warning(f"Error processing sample {idx}: {e}")
                continue


# ============== Video-only Datasets ==============

class Kinetics400Downloader(DatasetDownloader):
    """Kinetics-400 åŠ¨ä½œè¯†åˆ«æ•°æ®é›†"""
    
    name = "kinetics400"
    modality = "video"
    description = "äººç±»åŠ¨ä½œè¯†åˆ«æ•°æ®é›†ï¼Œ400ç±»"
    url = "https://github.com/cvdfoundation/kinetics-dataset"
    
    def download(self) -> bool:
        try:
            # Kineticséœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œè¿™é‡Œæä¾›ä¸‹è½½è„šæœ¬è·¯å¾„
            logger.info("Kinetics-400 requires manual download.")
            logger.info("Please download from: https://github.com/cvdfoundation/kinetics-dataset")
            logger.info("Or use: pip install kinetics-dataset")
            
            # å°è¯•åŠ è½½æœ¬åœ°æ•°æ®
            video_dir = self.data_dir / "videos"
            if video_dir.exists() and any(video_dir.iterdir()):
                self.video_files = list(video_dir.glob("*.mp4"))[:self.max_samples]
                return True
            return False
        except Exception as e:
            logger.error(f"Error with Kinetics-400: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        for idx, video_path in enumerate(self.video_files):
            if idx >= self.max_samples:
                break
            
            # ä»æ–‡ä»¶åæå–æ ‡ç­¾
            label = video_path.stem.rsplit("_", 1)[0] if "_" in video_path.stem else ""
            
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=label,
                video=str(video_path),
                modality="video"
            )


class MSRVTTDownloader(DatasetDownloader):
    """MSR-VTT è§†é¢‘æè¿°æ•°æ®é›†"""
    
    name = "msrvtt"
    modality = "video"
    description = "è§†é¢‘æè¿°åŸºå‡†æ•°æ®é›†"
    url = "https://cove.thecvf.com/datasets/839"
    
    def download(self) -> bool:
        try:
            logger.info("MSR-VTT requires manual download.")
            logger.info("Please download from: https://cove.thecvf.com/datasets/839")
            
            video_dir = self.data_dir / "videos"
            annotations_file = self.data_dir / "annotations.json"
            
            if video_dir.exists() and annotations_file.exists():
                with open(annotations_file) as f:
                    self.annotations = json.load(f)
                self.video_dir = video_dir
                return True
            return False
        except Exception as e:
            logger.error(f"Error with MSR-VTT: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        sentences = self.annotations.get("sentences", [])
        
        for idx, item in enumerate(sentences):
            if idx >= self.max_samples:
                break
            
            video_id = item.get("video_id", "")
            video_path = self.video_dir / f"{video_id}.mp4"
            
            if video_path.exists():
                yield UnifiedSample(
                    id=f"{self.name}_{idx:05d}",
                    text=item.get("caption", ""),
                    video=str(video_path),
                    modality="video"
                )


class LongVideoBenchDownloader(DatasetDownloader):
    """LongVideoBench é•¿è§†é¢‘ç†è§£æ•°æ®é›†"""
    
    name = "longvideobench"
    modality = "video"
    description = "é•¿è§†é¢‘ç†è§£åŸºå‡†æ•°æ®é›†"
    url = "https://huggingface.co/datasets/longvideobench/LongVideoBench"
    
    def download(self) -> bool:
        try:
            from datasets import load_dataset
            logger.info("Loading LongVideoBench from HuggingFace...")
            self.dataset = load_dataset(
                "longvideobench/LongVideoBench",
                split="test",
                trust_remote_code=True
            )
            return True
        except Exception as e:
            logger.error(f"Error loading LongVideoBench: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        video_dir = self.data_dir / "videos"
        video_dir.mkdir(exist_ok=True)
        
        for idx, item in enumerate(self.dataset):
            if idx >= self.max_samples:
                break
            
            try:
                # è·å–é—®é¢˜å’Œç­”æ¡ˆä½œä¸ºæ–‡æœ¬
                question = item.get("question", "")
                
                yield UnifiedSample(
                    id=f"{self.name}_{idx:05d}",
                    text=question,
                    video=item.get("video_path", None),
                    modality="video"
                )
            except Exception as e:
                logger.warning(f"Error processing sample {idx}: {e}")
                continue


# ============== Mixed Datasets (Audio + Video) ==============

class VoxCelebDownloader(DatasetDownloader):
    """VoxCeleb éŸ³è§†é¢‘è¯´è¯äººæ•°æ®é›†"""
    
    name = "voxceleb"
    modality = "mixed"
    description = "éŸ³è§†é¢‘è¯´è¯äººè¯†åˆ«æ•°æ®é›†"
    url = "https://robots.ox.ac.uk/~vgg/data/voxceleb"
    
    def download(self) -> bool:
        try:
            logger.info("VoxCeleb requires manual download with agreement.")
            logger.info("Please visit: https://robots.ox.ac.uk/~vgg/data/voxceleb")
            
            # æ£€æŸ¥æœ¬åœ°æ•°æ®
            data_path = self.data_dir / "voxceleb1"
            if data_path.exists():
                self.data_path = data_path
                return True
            return False
        except Exception as e:
            logger.error(f"Error with VoxCeleb: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        # VoxCeleb æ•°æ®æ ¼å¼: id/video_id/clip_id.wav
        for idx, audio_file in enumerate(self.data_path.rglob("*.wav")):
            if idx >= self.max_samples:
                break
            
            # æŸ¥æ‰¾å¯¹åº”çš„è§†é¢‘æ–‡ä»¶
            video_file = audio_file.with_suffix(".mp4")
            
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=None,  # VoxCeleb æ²¡æœ‰æ–‡æœ¬
                audio=str(audio_file),
                video=str(video_file) if video_file.exists() else None,
                modality="mixed"
            )


class How2Downloader(DatasetDownloader):
    """How2 å¤šæ¨¡æ€æ•™å­¦è§†é¢‘æ•°æ®é›†"""
    
    name = "how2"
    modality = "mixed"
    description = "å¤šæ¨¡æ€æ•™å­¦è§†é¢‘æ•°æ®é›†"
    url = "https://srvk.github.io/how2-dataset/"
    
    def download(self) -> bool:
        try:
            logger.info("How2 dataset requires download from official site.")
            logger.info("Please visit: https://srvk.github.io/how2-dataset/")
            
            data_path = self.data_dir / "how2"
            if data_path.exists():
                self.data_path = data_path
                return True
            return False
        except Exception as e:
            logger.error(f"Error with How2: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        # How2 é€šå¸¸æœ‰ video_id.mp4, video_id.wav, video_id.txt
        txt_files = list(self.data_path.glob("*.txt"))
        
        for idx, txt_file in enumerate(txt_files):
            if idx >= self.max_samples:
                break
            
            video_id = txt_file.stem
            audio_file = txt_file.with_suffix(".wav")
            video_file = txt_file.with_suffix(".mp4")
            
            with open(txt_file, 'r', encoding='utf-8') as f:
                text = f.read().strip()
            
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=text,
                audio=str(audio_file) if audio_file.exists() else None,
                video=str(video_file) if video_file.exists() else None,
                modality="mixed"
            )


class LRS2Downloader(DatasetDownloader):
    """LRS2 éŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«æ•°æ®é›†"""
    
    name = "lrs2"
    modality = "mixed"
    description = "BBCéŸ³è§†é¢‘è¯­éŸ³è¯†åˆ«æ•°æ®é›†"
    url = "https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html"
    
    def download(self) -> bool:
        try:
            logger.info("LRS2 requires agreement with BBC R&D.")
            logger.info("Please visit: https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html")
            
            data_path = self.data_dir / "lrs2"
            if data_path.exists():
                self.data_path = data_path
                return True
            return False
        except Exception as e:
            logger.error(f"Error with LRS2: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        # LRS2 æ ¼å¼: {split}/{video_id}/{clip_id}.mp4 + .txt
        mp4_files = list(self.data_path.rglob("*.mp4"))
        
        for idx, video_file in enumerate(mp4_files):
            if idx >= self.max_samples:
                break
            
            txt_file = video_file.with_suffix(".txt")
            text = ""
            if txt_file.exists():
                with open(txt_file, 'r', encoding='utf-8') as f:
                    text = f.read().strip()
            
            # éŸ³é¢‘åµŒå…¥åœ¨è§†é¢‘ä¸­
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=text,
                audio=None,  # éŸ³é¢‘åœ¨è§†é¢‘å†…
                video=str(video_file),
                modality="mixed"
            )


class AudioSetDownloader(DatasetDownloader):
    """AudioSet å¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†"""
    
    name = "audioset"
    modality = "mixed"
    description = "å¤§è§„æ¨¡éŸ³é¢‘äº‹ä»¶åˆ†ç±»æ•°æ®é›†"
    url = "https://research.google.com/audioset/"
    
    def download(self) -> bool:
        try:
            logger.info("AudioSet requires downloading from YouTube using official tools.")
            logger.info("Please visit: https://research.google.com/audioset/download.html")
            
            # æ£€æŸ¥æœ¬åœ°æ•°æ®
            data_path = self.data_dir / "audioset"
            if data_path.exists():
                self.data_path = data_path
                # å°è¯•åŠ è½½æ ‡ç­¾æ–‡ä»¶
                labels_file = data_path / "balanced_train_segments.csv"
                if labels_file.exists():
                    self.labels = self._load_labels(labels_file)
                    return True
            return False
        except Exception as e:
            logger.error(f"Error with AudioSet: {e}")
            return False
    
    def _load_labels(self, labels_file: Path) -> Dict:
        """åŠ è½½AudioSetæ ‡ç­¾æ–‡ä»¶"""
        labels = {}
        with open(labels_file, 'r') as f:
            for line in f:
                if line.startswith('#'):
                    continue
                parts = line.strip().split(',')
                if len(parts) >= 4:
                    ytid = parts[0].strip('"')
                    labels[ytid] = parts[3:]
        return labels
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        audio_files = list(self.data_path.glob("*.wav")) + list(self.data_path.glob("*.flac"))
        
        for idx, audio_file in enumerate(audio_files):
            if idx >= self.max_samples:
                break
            
            ytid = audio_file.stem
            label_ids = self.labels.get(ytid, [])
            
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=",".join(label_ids),
                audio=str(audio_file),
                video=None,
                modality="mixed"
            )


class VGGSoundDownloader(DatasetDownloader):
    """VGGSound éŸ³è§†é¢‘å¯¹åº”æ•°æ®é›†"""
    
    name = "vggsound"
    modality = "mixed"
    description = "éŸ³è§†é¢‘å¯¹åº”æ•°æ®é›†"
    url = "https://www.robots.ox.ac.uk/~vgg/data/vggsound/"
    
    def download(self) -> bool:
        try:
            logger.info("VGGSound requires downloading from official site.")
            logger.info("Please visit: https://www.robots.ox.ac.uk/~vgg/data/vggsound/")
            
            data_path = self.data_dir / "vggsound"
            if data_path.exists():
                self.data_path = data_path
                return True
            return False
        except Exception as e:
            logger.error(f"Error with VGGSound: {e}")
            return False
    
    def convert(self) -> Generator[UnifiedSample, None, None]:
        video_files = list(self.data_path.glob("*.mp4"))
        
        for idx, video_file in enumerate(video_files):
            if idx >= self.max_samples:
                break
            
            # ä»æ–‡ä»¶åæå–æ ‡ç­¾ (æ ¼å¼: ytid_start_end_label.mp4)
            parts = video_file.stem.rsplit("_", 1)
            label = parts[-1] if len(parts) > 1 else ""
            
            yield UnifiedSample(
                id=f"{self.name}_{idx:05d}",
                text=label.replace("_", " "),
                audio=None,  # éŸ³é¢‘åœ¨è§†é¢‘å†…
                video=str(video_file),
                modality="mixed"
            )


# ============== Dataset Registry ==============

DATASET_REGISTRY: Dict[str, type] = {
    # Audio-only
    "librispeech": LibriSpeechDownloader,
    "common_voice": CommonVoiceDownloader,
    "gigaspeech": GigaSpeechDownloader,
    "wavcaps": WavCapsDownloader,
    
    # Video-only
    "kinetics400": Kinetics400Downloader,
    "msrvtt": MSRVTTDownloader,
    "longvideobench": LongVideoBenchDownloader,
    
    # Mixed
    "voxceleb": VoxCelebDownloader,
    "how2": How2Downloader,
    "lrs2": LRS2Downloader,
    "audioset": AudioSetDownloader,
    "vggsound": VGGSoundDownloader,
}


def list_datasets():
    """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†"""
    print("\n" + "=" * 80)
    print("REAP-OMNI æ”¯æŒçš„æ•°æ®é›†")
    print("=" * 80)
    
    audio_datasets = []
    video_datasets = []
    mixed_datasets = []
    
    for name, cls in DATASET_REGISTRY.items():
        info = {"name": name, "description": cls.description, "url": cls.url}
        if cls.modality == "audio":
            audio_datasets.append(info)
        elif cls.modality == "video":
            video_datasets.append(info)
        else:
            mixed_datasets.append(info)
    
    print("\nğŸ“¢ Audio-only æ•°æ®é›† (S1):")
    print("-" * 40)
    for ds in audio_datasets:
        print(f"  â€¢ {ds['name']}: {ds['description']}")
        print(f"    URL: {ds['url']}")
    
    print("\nğŸ¬ Video-only æ•°æ®é›† (S2):")
    print("-" * 40)
    for ds in video_datasets:
        print(f"  â€¢ {ds['name']}: {ds['description']}")
        print(f"    URL: {ds['url']}")
    
    print("\nğŸ”€ Mixed æ•°æ®é›† (S3):")
    print("-" * 40)
    for ds in mixed_datasets:
        print(f"  â€¢ {ds['name']}: {ds['description']}")
        print(f"    URL: {ds['url']}")
    
    print("\n" + "=" * 80)


def save_jsonl(samples: List[UnifiedSample], output_path: Path, modality: str):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    output_file = output_path / f"{modality}.jsonl"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample.to_dict(), ensure_ascii=False) + '\n')
    
    logger.info(f"Saved {len(samples)} samples to {output_file}")
    return output_file


def process_datasets(
    datasets: List[str],
    output_dir: Path,
    max_samples: int = 100
) -> Dict[str, List[UnifiedSample]]:
    """å¤„ç†å¤šä¸ªæ•°æ®é›†"""
    
    results = {
        "audio": [],
        "video": [],
        "mixed": []
    }
    
    for ds_name in datasets:
        if ds_name not in DATASET_REGISTRY:
            logger.warning(f"Unknown dataset: {ds_name}")
            continue
        
        downloader_cls = DATASET_REGISTRY[ds_name]
        downloader = downloader_cls(output_dir, max_samples)
        
        try:
            samples = downloader.process()
            results[downloader.modality].extend(samples)
        except Exception as e:
            logger.error(f"Error processing {ds_name}: {e}")
            continue
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description="REAP-OMNI å¤šæ¨¡æ€æ•°æ®é›†ä¸‹è½½ä¸è½¬æ¢å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†
  python download_datasets.py --list
  
  # ä¸‹è½½å•ä¸ªæ•°æ®é›†
  python download_datasets.py --dataset librispeech --output ./data --samples 100
  
  # ä¸‹è½½å¤šä¸ªæ•°æ®é›†
  python download_datasets.py --dataset librispeech gigaspeech --output ./data
  
  # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
  python download_datasets.py --dataset all --output ./data
  
  # æŒ‰æ¨¡æ€ä¸‹è½½
  python download_datasets.py --modality audio --output ./data
        """
    )
    
    parser.add_argument(
        "--list", "-l",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ•°æ®é›†"
    )
    
    parser.add_argument(
        "--dataset", "-d",
        nargs="+",
        default=[],
        help="è¦ä¸‹è½½çš„æ•°æ®é›†åç§°ï¼Œä½¿ç”¨ 'all' ä¸‹è½½å…¨éƒ¨"
    )
    
    parser.add_argument(
        "--modality", "-m",
        choices=["audio", "video", "mixed", "all"],
        default=None,
        help="æŒ‰æ¨¡æ€ç±»å‹ä¸‹è½½"
    )
    
    parser.add_argument(
        "--output", "-o",
        type=Path,
        default=Path("./data"),
        help="è¾“å‡ºç›®å½•"
    )
    
    parser.add_argument(
        "--samples", "-s",
        type=int,
        default=100,
        help="æ¯ä¸ªæ•°æ®é›†çš„æœ€å¤§æ ·æœ¬æ•°"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—"
    )
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    if args.list:
        list_datasets()
        return
    
    # ç¡®å®šè¦å¤„ç†çš„æ•°æ®é›†
    datasets_to_process = []
    
    if args.modality:
        if args.modality == "all":
            datasets_to_process = list(DATASET_REGISTRY.keys())
        else:
            for name, cls in DATASET_REGISTRY.items():
                if cls.modality == args.modality:
                    datasets_to_process.append(name)
    elif args.dataset:
        if "all" in args.dataset:
            datasets_to_process = list(DATASET_REGISTRY.keys())
        else:
            datasets_to_process = args.dataset
    else:
        parser.print_help()
        return
    
    if not datasets_to_process:
        logger.error("No datasets to process!")
        return
    
    logger.info(f"Processing datasets: {datasets_to_process}")
    logger.info(f"Output directory: {args.output}")
    logger.info(f"Max samples per dataset: {args.samples}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output.mkdir(parents=True, exist_ok=True)
    calibration_dir = args.output / "calibration"
    calibration_dir.mkdir(exist_ok=True)
    
    # å¤„ç†æ•°æ®é›†
    results = process_datasets(datasets_to_process, args.output, args.samples)
    
    # ä¿å­˜ä¸ºJSONL
    for modality, samples in results.items():
        if samples:
            save_jsonl(samples, calibration_dir, modality)
    
    # æ‰“å°æ‘˜è¦
    print("\n" + "=" * 80)
    print("å¤„ç†å®Œæˆ!")
    print("=" * 80)
    print(f"  Audio samples: {len(results['audio'])}")
    print(f"  Video samples: {len(results['video'])}")
    print(f"  Mixed samples: {len(results['mixed'])}")
    print(f"\næ ¡å‡†æ•°æ®å·²ä¿å­˜åˆ°: {calibration_dir}")
    print("\nå¯ç”¨äº REAP-OMNI çš„å‘½ä»¤:")
    print(f"  python reap_expert_pruning.py \\")
    print(f"      --audio-data {calibration_dir / 'audio.jsonl'} \\")
    print(f"      --video-data {calibration_dir / 'video.jsonl'} \\")
    print(f"      --mixed-data {calibration_dir / 'mixed.jsonl'}")


if __name__ == "__main__":
    main()
