# Paper ID: SING: Spatial Context in Large Language Model for Next-Gen Wearables (arXiv:2504.08907)

## 1. 核心任务 (Task Definition)
- **现存问题**: Current wearable assistants (glasses, earbuds) lack "Spatial Awareness". They hear audio but don't know *where* it comes from or *who* is speaking relative to the user.
- **论文解决**: Proposed "SING", a system incorporating spatial speech understanding into LLMs for wearables.
- **意义**: Enables "Look-at-to-talk" or "Focus-on-speaker" interactions, essential for AR glasses and smart hearing aids.

## 2. 方法论归类 (Methodology Class)
- **核心类**: Multimodal (Audio + Spatial + Text) / Wearable AI.
- **核心机制**:
    - **Microstructure Sensing**: Extracts Direction of Arrival (DoA) using monaural microphones (novel hardware usage).
    - **Modality Fusion**: Fuses Whisper linguistic embeddings with spatial embeddings.
    - **LoRA Adaptation**: Fine-tunes LLaMA-3.2 3B on this fused input for on-device inference.
- **创新点**: Integrating "Spatial" as a first-class modality into the LLM context window.

## 3. 贡献总结 (Contribution)
- **Accuracy**: DoA error reduced to 25.72 degrees (vs 88.52 median in baselines).
- **Soundscaping**: Can infer "5 people talking, one at 30 degrees, one at -45 degrees".
- **Dataset**: Created "OmniTalk" synthetic spatial dataset.
